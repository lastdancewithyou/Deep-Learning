{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5hwL5-Skhug"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYs3EXAP7EKx"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/감성분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lq6mMVT7EIf"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VRVF4IN7EGB"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J10QQ5NjH8bJ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 텍스트 전처리 함수 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA3_DaPQJwY0"
      },
      "outputs": [],
      "source": [
        "# 한글만 추출하기\n",
        "def hangul_only(df : pd.DataFrame, convert_column : str) -> pd.DataFrame:\n",
        "    df['sentence'] = df[convert_column].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9 ]\",\"\")\n",
        "    df['sentence'] = df['sentence'].replace('^ +', '')\n",
        "    df['sentence'].replace('', np.nan, inplace = True)\n",
        "    df = df.dropna(how = 'any')\n",
        "    return df\n",
        "\n",
        "# 형태소 분석 -> mecab 이용해서 명사, 형용사, 동사만 추출\n",
        "def mecab_preprocessing(df : pd.DataFrame, convert_column : str) -> pd.DataFrame:\n",
        "    tags = ['JK', 'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC', 'EP', 'EF', 'EC', 'ETN', 'ETM']\n",
        "\n",
        "    from konlpy.tag import Mecab\n",
        "\n",
        "    mecab = Mecab()\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), desc = 'removing josa', total = len(df)):\n",
        "        josa_removed = [x[0] for x in mecab.pos(row['sentence']) if x[1] not in tags]\n",
        "        df.loc[idx, 'preprocessed_sentence'] = ' '.join(josa_removed)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 데이터 간소화 하기 (필요한 것만 불러오기)\n",
        "def get_data_only_review(df : pd.DataFrame) -> pd.DataFrame:\n",
        "    return df[['sentence', 'preprocessed_sentence', 'label']]\n",
        "\n",
        "# 각 데이터 합치기\n",
        "def concat_individual_data(data_list : list) -> pd.DataFrame:\n",
        "    df = data_list[0].copy()\n",
        "    for i in range(1, len(data_list)):\n",
        "        df = pd.concat([df, data_list[i]], axis = 0).reset_index(drop = True)\n",
        "    return df.reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vachdvUhJW_E"
      },
      "outputs": [],
      "source": [
        "review_df = pd.read_csv('mecab_data.csv').drop(['index', 'id'], axis = 1) ## 영화 리뷰 데이터\n",
        "review_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN9FcGdlJW8t"
      },
      "outputs": [],
      "source": [
        "shopping_df = pd.read_csv('shopping_review.csv') ## 쇼핑 리뷰 데이터\n",
        "shopping_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjwVdhI1JW6U"
      },
      "outputs": [],
      "source": [
        "shopping_df['label'] = shopping_df['ratings'].apply(lambda x : 1 if x >= 4 else 0)\n",
        "shopping_df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BDrGzM0JW3u"
      },
      "outputs": [],
      "source": [
        "## get_data_only_review => 필요한 것만 불러오기\n",
        "review_data = get_data_only_review(review_df)\n",
        "shopping_data = get_data_only_review(shopping_df)\n",
        "\n",
        "## 영화 리뷰, 쇼핑 리뷰 데이터 합치기\n",
        "train_data = concat_individual_data([review_data, shopping_data])\n",
        "print('합쳐진 데이터의 개수 : {}'.format(len(train_data)))\n",
        "print('셔플 전 처음 세 개 : ', train_data.head(3))\n",
        "\n",
        "train_data = train_data.sample(frac = 1).reset_index(drop = True)\n",
        "print('셔플 후 처음 세 개 : ', train_data.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuSYES8-JW1K"
      },
      "outputs": [],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AtlobMoJWyP"
      },
      "outputs": [],
      "source": [
        "## 클래스 라벨 비율 분포 확인\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.bar(train_data['label'].value_counts().index, train_data['label'].value_counts(), color = 'skyblue')\n",
        "plt.title('label distribution of data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXhx3TeHJWvM"
      },
      "outputs": [],
      "source": [
        "document_length = [len(x) for x in train_data['preprocessed_sentence'].astype(str)]\n",
        "plt.hist(document_length, bins = 30)\n",
        "plt.title('document length distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJB7veyb7EDl"
      },
      "outputs": [],
      "source": [
        "numpy_document_length = np.array(document_length)\n",
        "print('전체 문서의 (평균, 표준편차) : ({}, {}):'.format(numpy_document_length.mean(), numpy_document_length.std()))\n",
        "print('전체 문서의 (최소, 최대) : ({}, {})'.format(numpy_document_length.min(), numpy_document_length.max()))\n",
        "print('전체 문서의 개수 : {}'.format(len(document_length)))\n",
        "print('문서의 길이가 3보다 큰 것의 개수 : {}'.format(len(numpy_document_length[numpy_document_length > 3])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZNJWzxA_dn1"
      },
      "outputs": [],
      "source": [
        "with open('review_1120.txt', 'w', encoding = 'utf8') as f:\n",
        "    f.write('\\n'.join(train_data['sentence']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRPZwsKP_dlw"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "spm.SentencePieceTrainer.Train('--input=review_1120.txt \\\n",
        "--model_prefix=tokenizer --vocab_size=32000 --model_type=bpe --max_sentence_length=9999')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdTtTxhvSr1s"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import sentencepiece as spm\n",
        "\n",
        "vocab_list = pd.read_csv('tokenizer.vocab', sep = '\\t', header = None, quoting = csv.QUOTE_NONE)\n",
        "print(vocab_list.head(10))\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"tokenizer.model\"\n",
        "sp.load(vocab_file)\n",
        "\n",
        "lines = [\n",
        "  \"뭐 이딴 것도 영화냐.\",\n",
        "  \"진짜 최고의 영화입니다 ㅋㅋ\",\n",
        "  \"커버력 좋아서 투명한 피부로 보이게 만들어 줌 피지도 잘 가려줌 밀착력도 있고 보송보송함\"\n",
        "]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOR97IX3_djb"
      },
      "outputs": [],
      "source": [
        "train_data['sentence_tokenized'] = [' '.join(sp.encode_as_pieces(line)) for line in train_data['sentence']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwpeqtyi_dhE"
      },
      "outputs": [],
      "source": [
        "train_data['공백 길이'] = train_data['sentence'].apply(lambda x : len(x.split(' ')))\n",
        "train_data[train_data['공백 길이'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux-832trSb9W"
      },
      "outputs": [],
      "source": [
        "train = train_data[train_data['공백 길이'] >= 3].reset_index(drop = True)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6PIqePd8hIb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "X = train.drop('label', axis = 1)\n",
        "y = train['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 2022)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "\n",
        "train_X = []\n",
        "test_X = []\n",
        "\n",
        "for sentence in tqdm(X_train['sentence_tokenized']):\n",
        "    tokenized_sentence = sentence.split(' ')\n",
        "    train_X.append(tokenized_sentence)\n",
        "\n",
        "for sentence in tqdm(X_test['sentence_tokenized']):\n",
        "    tokenized_sentence = sentence.split(' ')\n",
        "    test_X.append(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0942tIG8hGF"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(oov_token = '<OOV>') ## 토큰화 한 것을 정수형으로\n",
        "tokenizer.fit_on_texts(train_X)\n",
        "print(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okzY1yVw8g0Y"
      },
      "outputs": [],
      "source": [
        "# saving\n",
        "with open('tokenizer_1120.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhEtv5wxTpdH"
      },
      "outputs": [],
      "source": [
        "threshold = 20\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgsFR0YPTpay"
      },
      "outputs": [],
      "source": [
        "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
        "# 0번 패딩 토큰을 고려하여 + 1\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기 :',vocab_size)\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size, oov_token = '<OOV>')\n",
        "tokenizer.fit_on_texts(train_X)\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "test_X = tokenizer.texts_to_sequences(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnDweLdzTpYb"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMIq8cqyTpWH"
      },
      "outputs": [],
      "source": [
        "print('리뷰의 최대 길이 :',max(len(review) for review in train_X))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, train_X))/len(train_X))\n",
        "plt.hist([len(review) for review in train_X], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no3w_iyiT3LQ"
      },
      "outputs": [],
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "        count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRRu1VBfT3JL"
      },
      "outputs": [],
      "source": [
        "max_len = 50\n",
        "below_threshold_len(max_len, train_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1o4KVd-T3HG"
      },
      "outputs": [],
      "source": [
        "train_X = pad_sequences(train_X, maxlen=max_len)\n",
        "test_X = pad_sequences(test_X, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM 모델 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKsQebAhT3Et"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.experimental import CosineDecay\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(128, activation = LeakyReLU(alpha = 0.03)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation = LeakyReLU(alpha = 0.03)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=3)\n",
        "mc = ModelCheckpoint('sentiment_model_1120.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
        "#history = model.fit(X_train, y_train, epochs=30, callbacks = [es, mc], batch_size=128, validation_split = 0.2)\n",
        "history = model.fit(train_X, y_train, epochs = 30, callbacks = [es,mc], batch_size = 128, validation_data = [test_X, y_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODDyHfC2U3sr"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model('sentiment_model_1120.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(test_X, y_test)[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPLIj1tbU_vP"
      },
      "outputs": [],
      "source": [
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "     pickle.dump(tokenizer, handle)\n",
        "\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91rMz2saUuIs"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vN_vsC_VE6g"
      },
      "outputs": [],
      "source": [
        "stopwords = pd.read_excel('한국어 불용어 목록.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veiY9SKQT3CI"
      },
      "outputs": [],
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5) :\n",
        "    result = \"긍정 리뷰\"\n",
        "\n",
        "  else :\n",
        "    result = \"부정 리뷰\"\n",
        "  return result, score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 훈련된 모델을 새로운 리뷰 데이터에 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKNeWHqiT2_h"
      },
      "outputs": [],
      "source": [
        "review = pd.read_csv('Review.csv')\n",
        "review.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq0sU-H5eN6M"
      },
      "outputs": [],
      "source": [
        "reviews = review[us_review['Type'] == 'Order']\n",
        "\n",
        "selected_columns = ['Order ID', 'User ID', 'Rating', 'Content']\n",
        "review_data = reviews[selected_columns]\n",
        "review_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrWJqmNNugLc"
      },
      "outputs": [],
      "source": [
        "sentiment_results = []\n",
        "okt = Okt()\n",
        "\n",
        "for index, row in review_data.iterrows():\n",
        "    sentence = row['Content']\n",
        "    user_id = row['User ID']\n",
        "    order_id = row['Order ID']\n",
        "    rating = row['Rating']\n",
        "\n",
        "    result, score = sentiment_predict(sentence)\n",
        "    print(\"User ID : \", user_id, \"Order ID : \", order_id, \"리뷰 : \", sentence, \"결과 : \", result, \"점수 : \", score, \"평점 : \", rating)\n",
        "\n",
        "    sentiment_results.append({\n",
        "        'User ID': user_id,\n",
        "        'Order ID': order_id,\n",
        "        'sentence': sentence,\n",
        "        'sentiment': result,\n",
        "        'score': score,\n",
        "        'rating':rating\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0ytgIHMT29K"
      },
      "outputs": [],
      "source": [
        "sentiment_results= pd.DataFrame(sentiment_results)\n",
        "sentiment_results.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
